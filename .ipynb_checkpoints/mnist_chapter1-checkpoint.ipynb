{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data, training_inputs, training_results)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning algorithm for a feedforward neural network. \n",
    "Gradients are calculated using backpropagation. Note that I have focused on making the code simple, easily\n",
    "readable, and easily modifiable. It is not optimized, and omits many desirable features.\n",
    "'''\n",
    "\n",
    "#### Libraries\n",
    "# Standard Library\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "\n",
    "class FeedForwardNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        '''\n",
    "        The list ```sizes``` contains the number of neurons in the respective layers of the network.\n",
    "        For example, if the list was [2,3,1] then it would be a three-layer network,\n",
    "        with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer\n",
    "        1 neuron. The biases and weights for the network are initialized randomly, using a \n",
    "        Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed\n",
    "        to be an input layer, and by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later layers.\n",
    "        '''\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                           for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.counter = 0\n",
    "#         matrix_weight1 = np.random.randn(30, 784)\n",
    "#         matrix_weight2 = np.random.randn(10, 30)\n",
    "#         self.matrix_weights = [matrix_weight1, matrix_weight2]\n",
    "        \n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        '''Return the output of the network if \"a\" is input.'''\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic gradient descent. \n",
    "        The 'training_data' is a list of tuples '(x,y)' representing the training inputs\n",
    "        and the desired outputs. The other non-optional parameters are self explanatory. If 'test_data' os provided\n",
    "        then the network will be evaluated against the test data after each epoch, and partial progress printed out.\n",
    "        The evaluating on test_data + printing is useful for tracking progress, but slows things down substantially\"\"\"\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, learning_rate) # Use this for looping method\n",
    "#                 self.matrix_update_mini_batch(mini_batch, learning_rate) # Use this for matrix calculation method\n",
    "            if test_data:\n",
    "                correct_results = self.evaluate(test_data)\n",
    "                percentage = correct_results/n_test\n",
    "                print(\"Epoch {0}: {1} / {2} ---> {3}\".format(\n",
    "                    j, self.evaluate(test_data), n_test, percentage))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, learning_rate):\n",
    "        '''\n",
    "        Update the network's weights and biases by applying gradient descent using backprop\n",
    "        to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x,y)\"\n",
    "        '''\n",
    "        # 2/11/2024 TODO figure out what nabala is for and get rid of it\n",
    "        # 2/12/2024 I do not think I can get rid of it. Its used for mini batch updates I believe\n",
    "            # it seems to be used to get the accumlulated error for our parameters\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "#         print('mini_batch[0]',mini_batch[0])\n",
    "        for x,y in mini_batch:\n",
    "#             print('x in update_single_mini_batch',x.shape)\n",
    "#             print('y in update_single_mini_batch',y.shape)\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            self.weights = [w-(learning_rate/len(mini_batch)) * nw\n",
    "                               for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b-(learning_rate/len(mini_batch)) * nb\n",
    "                            for b, nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "    def matrix_update_mini_batch(self, mini_batch, learning_rate):\n",
    "        '''\n",
    "        Update the network's weights and biases by applying gradient descent using backprop\n",
    "        to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x,y)\"\n",
    "        '''\n",
    "        xs, ys = zip(*mini_batch)\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        delta_b, delta_w = self.matrix_backprop(xs, ys)\n",
    "#         print('delta_b',delta_b)\n",
    "#         print('delta_w',delta_w)\n",
    "#         for w, nw in zip(self.weights, delta_w):\n",
    "#             print('w.shape',w.shape)\n",
    "#             np_nw = np.array(nw)\n",
    "#             print('np_nw.shape',np_nw.shape)\n",
    "#         for b, nb in zip(self.biases, delta_b):\n",
    "#             print('b.shape',b.shape)\n",
    "#             np_nb = np.array(nb)\n",
    "#             print('np_nb.shape',np_nb.shape)\n",
    "        \n",
    "        # The error is in the way I update these weights. \n",
    "        # TODO: Look up how to update weights in mini batch learning\n",
    "        # the dimensions seem to line up everywhere my intuition tells me the issue is in the updates\n",
    "        self.weights = [w-(learning_rate/len(mini_batch)) * nw\n",
    "                               for w, nw in zip(self.weights, delta_w)]\n",
    "        self.biases = [b-(learning_rate/len(mini_batch)) * nb\n",
    "                              for b, nb in zip(self.biases, delta_b)]\n",
    "        # TODO: When I come back compare the sizes of self.biases, self.weights, and delta_w and delta_b\n",
    "        # Does the sizes and the update make sense? Is it doing the right thing?\n",
    "#         print('delta_b',delta_b)\n",
    "#         print('delta_w',delta_w)\n",
    "#         np_delta_b = np.array(delta_b)\n",
    "#         np_delta_w = np.array(delta_w)\n",
    "#         print('np_delta_b.shape',np_delta_b.shape)\n",
    "#         print('np_delta_w.shape',np_delta_w.shape)\n",
    "#         print('delta_b.shape',len(delta_b))\n",
    "#         print('delta_w.shape',len(delta_w))\n",
    "#         print('xs.shape',xs.shape)\n",
    "#         print('ys.shape',ys.shape)\n",
    "#         for x,y in mini_batch:\n",
    "#             delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "#             nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "#             nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "#             self.weights = [w-(learning_rate/len(mini_batch)) * nw\n",
    "#                                for w, nw in zip(self.weights, nabla_w)]\n",
    "#             self.biases = [b-(learning_rate/len(mini_batch)) * nb\n",
    "#                               for b, nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "    def backprop(self, x, y):\n",
    "        '''\n",
    "        This does not get explained in chapter 1.\n",
    "        Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x. ``nabla_b`` and \n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays similar to\n",
    "        ``self.biases`` and ``self.weights``.\n",
    "        '''\n",
    "        # TODO: Understand this better in Chapter 2!!!\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        # Why are we doing a feedforward in the backprop?\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = [] # list to store all of the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "#             print('w.shape',w.shape)\n",
    "#             print('b.shape',b.shape)\n",
    "#             print('activation.shape',activation.shape)\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # Backwards pass\n",
    "        # memorize code and write this from memory once you feel like you understand it\n",
    "        delta = sigmoid_prime(zs[-1]) * \\\n",
    "                    self.cost_derivative(activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "#         print('delta.shape',delta.shape)\n",
    "#         print('activations[-2].transpose().shape',activations[-2].transpose().shape)\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the \n",
    "        # second-last layer, and so on. It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers): \n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "#             print('delta.shape in for loop',delta.shape)\n",
    "#             print('activations[-l-1].transpose().shape',activations[-l-1].transpose().shape)\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def matrix_backprop(self, xs, ys):\n",
    "        '''\n",
    "        This does not get explained in chapter 1.\n",
    "        Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x. ``nabla_b`` and \n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays similar to\n",
    "        ``self.biases`` and ``self.weights``.\n",
    "        '''\n",
    "        # TODO: Understand this better in Chapter 2!!!\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        # Why are we doing a feedforward in the backprop?\n",
    "        activation = xs.view().reshape(10, -1).T\n",
    "        activations = [activation]\n",
    "#         print('xs.shape',xs.shape)\n",
    "#         print('activation.shape',activation.shape)\n",
    "#         print('self.biases[0].shape',self.biases[0].shape)\n",
    "#         print('self.weights[0].shape',self.matrix_weights[0].shape)\n",
    "        zs = [] # list to store all of the z vectors, layer by layer\n",
    "    \n",
    "        z1 = np.dot(self.weights[0], activation) # (30, 784) * (784,10)\n",
    "#         print('self.weights[0].shape',self.weights[0].shape)\n",
    "#         print('1st activation.shape',activation.shape)\n",
    "#         print('self.biases[0].shape',self.biases[0].shape)\n",
    "        z1 += self.biases[0] # (30,10) + (30,1)\n",
    "        zs.append(z1)\n",
    "        activation = sigmoid(z1)\n",
    "        activations.append(activation)\n",
    "        z2 = np.dot(self.weights[1], activation) # (10,30) * (30,10)\n",
    "#         print('self.weights[1].shape',self.weights[1].shape)\n",
    "#         print('self.biases[1].shape',self.biases[1].shape)\n",
    "#         print('2nd activation.shape',activation.shape)\n",
    "        z2 += self.biases[1] # (10,10) + (10,1)\n",
    "        zs.append(z2)\n",
    "        activation = sigmoid(z2) # (10,10) output\n",
    "        activations.append(activation)\n",
    "#         print('final activation.shape',activation.shape)\n",
    "#         for b, w in zip(self.biases, self.weights):\n",
    "#             z = np.dot(w, activation) #()\n",
    "#             z += b\n",
    "#             zs.append(z)\n",
    "#             activation = sigmoid(z)\n",
    "#             activations.append(activation)\n",
    "        # Backwards pass\n",
    "        # memorize code and write this from memory once you feel like you understand it\n",
    "#         print('ys.shape',ys.shape)\n",
    "        reshaped_ys = ys.view().reshape(10,10)\n",
    "#         print('sigmoid_prime(zs[-1]).shape',sigmoid_prime(zs[-1]).shape)\n",
    "#         print('self.matrix_cost_derivative(activations[-1], reshaped_ys).shape',self.matrix_cost_derivative(activations[-1], reshaped_ys).shape)\n",
    "        delta = sigmoid_prime(zs[-1]) * \\\n",
    "                    self.matrix_cost_derivative(activations[-1], reshaped_ys)\n",
    "#         delta = delta.sum(axis=1,keepdims=True)\n",
    "        nabla_b[-1] = delta.sum(axis=1,keepdims=True)\n",
    "#         print('delta.sum(axis=1)')\n",
    "#         print('delta.shape',delta.shape)\n",
    "#         print('activations[-2].transpose().shape',activations[-2].transpose().shape)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "#         print('new batch------------------------------')\n",
    "#         print('delta_b.shape',delta.shape)\n",
    "#         print('delta_w.shape',np.dot(delta, activations[-2].transpose()).shape)\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the \n",
    "        # second-last layer, and so on. It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers): \n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "#             print('delta.shape',delta.shape)\n",
    "#             delta = delta.sum(axis=1,keepdims=True)\n",
    "            nabla_b[-l] = delta.sum(axis=1,keepdims=True)\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "#             print('delta.shape',delta.shape)\n",
    "#             print('activations[-l-1].transpose().shape',activations[-l-1].transpose().shape)\n",
    "#             print('delta_b.shape',delta.shape)\n",
    "#             print('delta_w.shape',np.dot(delta, activations[-l-1].transpose()).shape)\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        '''\n",
    "        Return the number of test inputs for which the neural network outputs\n",
    "        the correct result. Note that the network's output is the index\n",
    "        of the neuron in the final layer with the highest activation\n",
    "        '''\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                           for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x,y) in test_results)\n",
    "        \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        '''\n",
    "        Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations\n",
    "        '''\n",
    "        # Why is the partial derivatives this subtraction?\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def matrix_cost_derivative(self, output_activations, ys):\n",
    "        '''\n",
    "        Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations\n",
    "        '''\n",
    "#         print('output_activations',output_activations)\n",
    "#         print('ys',ys)\n",
    "        # See what the predictions are for the neural network4\n",
    "#         if(self.counter == 0):\n",
    "#             self.counter += 1\n",
    "#         print('output_activations',output_activations)\n",
    "#         print('predictions', np.argmax(output_activations,axis=1))\n",
    "#         print('actual outputs',np.argmax(ys,axis=1))\n",
    "#         if(self.counter == 0):\n",
    "#             self.counter += 1\n",
    "#             print('output_activations.shape',output_activations.shape)\n",
    "#             print('ys.shape',ys.shape)\n",
    "#             print('(output_activations-ys).shape',(output_activations-ys).shape)\n",
    "#             print('output_activations',output_activations)\n",
    "\n",
    "#             print('output_activations prediction', np.argmax(output_activations,axis=1))\n",
    "#             print('ys actual outputs',np.argmax(ys,axis=1))\n",
    "#             print('output_activations',output_activations)\n",
    "#             print('ys',ys)\n",
    "#             print('output_activations-ys',(output_activations-ys))\n",
    "            \n",
    "        return (output_activations-ys)\n",
    "\n",
    "# Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    The sigmoid function\n",
    "    '''\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    '''\n",
    "    Derivative of the sigmoid function\n",
    "    '''\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data, training_inputs, training_results = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(training_data) <class 'list'>\n",
      "type(validation_data) <class 'list'>\n",
      "type(test_data) <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print('type(training_data)',type(training_data))\n",
    "print('type(validation_data)',type(validation_data))\n",
    "print('type(test_data)',type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains a 30 neuron hidden layer\n",
    "net = FeedForwardNetwork([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Hidden layer\n",
    "# net = FeedForwardNetwork([784, 10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 3247 / 10000 ---> 0.3247\n",
      "Epoch 1: 3717 / 10000 ---> 0.3717\n",
      "Epoch 2: 4803 / 10000 ---> 0.4803\n",
      "Epoch 3: 4861 / 10000 ---> 0.4861\n",
      "Epoch 4: 5114 / 10000 ---> 0.5114\n",
      "Epoch 5: 5666 / 10000 ---> 0.5666\n",
      "Epoch 6: 5504 / 10000 ---> 0.5504\n",
      "Epoch 7: 6760 / 10000 ---> 0.676\n",
      "Epoch 8: 7032 / 10000 ---> 0.7032\n",
      "Epoch 9: 6430 / 10000 ---> 0.643\n",
      "Epoch 10: 6951 / 10000 ---> 0.6951\n",
      "Epoch 11: 7322 / 10000 ---> 0.7322\n",
      "Epoch 12: 7348 / 10000 ---> 0.7348\n",
      "Epoch 13: 6701 / 10000 ---> 0.6701\n",
      "Epoch 14: 7728 / 10000 ---> 0.7728\n",
      "Epoch 15: 7805 / 10000 ---> 0.7805\n",
      "Epoch 16: 7848 / 10000 ---> 0.7848\n",
      "Epoch 17: 7808 / 10000 ---> 0.7808\n",
      "Epoch 18: 7966 / 10000 ---> 0.7966\n",
      "Epoch 19: 7929 / 10000 ---> 0.7929\n",
      "Epoch 20: 7984 / 10000 ---> 0.7984\n",
      "Epoch 21: 8140 / 10000 ---> 0.814\n",
      "Epoch 22: 8129 / 10000 ---> 0.8129\n",
      "Epoch 23: 8241 / 10000 ---> 0.8241\n",
      "Epoch 24: 7439 / 10000 ---> 0.7439\n",
      "Epoch 25: 8267 / 10000 ---> 0.8267\n",
      "Epoch 26: 8314 / 10000 ---> 0.8314\n",
      "Epoch 27: 8048 / 10000 ---> 0.8048\n",
      "Epoch 28: 8267 / 10000 ---> 0.8267\n",
      "Epoch 29: 8327 / 10000 ---> 0.8327\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
